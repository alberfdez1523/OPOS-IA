# ğŸ“ˆ Opos AI â€” Asistente RAG de Opisicones de MÃºsica

Un chatbot RAG (Retrieval-Augmented Generation) que responde preguntas sobre Oposiciones de Musica usando apuntes subidos

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend   â”‚â”€â”€â”€â–¶â”‚  FastAPI API  â”‚â”€â”€â”€â–¶â”‚   Ollama     â”‚
â”‚  HTML/CSS/JS â”‚    â”‚              â”‚    â”‚  Llama 3.1   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚   â”‚ FAISS  â”‚ â”‚
                   â”‚   â”‚ Vector â”‚ â”‚
                   â”‚   â”‚ Store  â”‚ â”‚
                   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flujo RAG:
1. **Pregunta** â†’ El usuario escribe una pregunta
2. **Retrieval** â†’ BÃºsqueda semÃ¡ntica en FAISS (embeddings HuggingFace)
3. **Augmentation** â†’ Los fragmentos relevantes se inyectan como contexto
4. **Generation** â†’ Llama 3.1 (vÃ­a Ollama) genera la respuesta

## ğŸ“‹ Requisitos

- **Python 3.11+**
- **Ollama** con el modelo `llama3.1` instalado
- ~4GB RAM para embeddings + FAISS
- ~8GB RAM para Llama 3.1 (8B)

## ğŸš€ InstalaciÃ³n

### 1. Instalar Ollama

```bash
# Windows: descargar de https://ollama.com/download
# Luego descargar el modelo:
ollama pull llama3.1
```

### 2. Configurar el backend

```bash
cd backend

# Crear entorno virtual
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac

# Instalar dependencias
pip install -r requirements.txt

# Copiar configuraciÃ³n (editar si necesario)
copy .env.example .env
```

### 3. AÃ±adir PDFs

Colocar los archivos PDF del curso en:
```
backend/data/pdfs/
```

### 4. Ejecutar

```bash
cd backend

# Iniciar el servidor
python -m uvicorn app.main:app --reload --port 8000
```

### 5. Indexar documentos

Una vez el servidor estÃ© corriendo y los PDFs estÃ©n en `data/pdfs/`:

- **OpciÃ³n A**: Desde la interfaz web, pulsar "ğŸ”„ Indexar documentos"
- **OpciÃ³n B**: Llamar al API directamente:
  ```bash
  curl -X POST http://localhost:8000/api/ingest
  ```

### 6. Abrir la web

Ir a: **http://localhost:8000**

## ğŸ“ Estructura del Proyecto

```
timeseries-ai/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py           # FastAPI app principal
â”‚   â”‚   â”œâ”€â”€ config.py         # ConfiguraciÃ³n
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â””â”€â”€ routes.py     # Endpoints REST
â”‚   â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”‚   â”œâ”€â”€ embeddings.py # Modelo de embeddings
â”‚   â”‚   â”‚   â”œâ”€â”€ vectorstore.py# FAISS index
â”‚   â”‚   â”‚   â”œâ”€â”€ retriever.py  # BÃºsqueda semÃ¡ntica
â”‚   â”‚   â”‚   â””â”€â”€ llm.py        # IntegraciÃ³n Llama 3.1
â”‚   â”‚   â””â”€â”€ ingestion/
â”‚   â”‚       â””â”€â”€ pdf_loader.py # Pipeline de ingesta PDF
â”‚   â”œâ”€â”€ data/pdfs/            # â† Colocar PDFs aquÃ­
â”‚   â”œâ”€â”€ vectorstore/          # Ãndice FAISS (auto-generado)
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env
â””â”€â”€ frontend/
    â”œâ”€â”€ index.html            # Landing page
    â”œâ”€â”€ chat.html             # Interfaz de chat
    â”œâ”€â”€ css/styles.css        # Estilos
    â””â”€â”€ js/
        â”œâ”€â”€ main.js           # JS landing
        â””â”€â”€ chat.js           # JS chat + streaming
```

## ğŸ”Œ API Endpoints

| MÃ©todo | Ruta | DescripciÃ³n |
|--------|------|-------------|
| `GET` | `/api/stats` | EstadÃ­sticas del sistema |
| `POST` | `/api/ask` | Pregunta (respuesta completa) |
| `POST` | `/api/ask/stream` | Pregunta (respuesta streaming SSE) |
| `POST` | `/api/ingest` | Indexar PDFs de `data/pdfs/` |
| `POST` | `/api/upload` | Subir un PDF |

## âš™ï¸ ConfiguraciÃ³n (.env)

| Variable | Default | DescripciÃ³n |
|----------|---------|-------------|
| `OLLAMA_BASE_URL` | `http://localhost:11434` | URL de Ollama |
| `OLLAMA_MODEL` | `llama3.1` | Modelo LLM |
| `EMBEDDING_MODEL` | `sentence-transformers/all-MiniLM-L6-v2` | Modelo embeddings |
| `CHUNK_SIZE` | `500` | TamaÃ±o de chunks |
| `CHUNK_OVERLAP` | `50` | Overlap entre chunks |

## ğŸ¨ CaracterÃ­sticas

- âœ… Landing page con diseÃ±o dark mode
- âœ… GrÃ¡fico de serie temporal animado en tiempo real
- âœ… Chat con streaming (Server-Sent Events)
- âœ… Renderizado de Markdown y LaTeX (KaTeX)
- âœ… BÃºsqueda semÃ¡ntica con FAISS
- âœ… Subida de PDFs desde la interfaz
- âœ… Muestra las fuentes consultadas en cada respuesta
- âœ… Preguntas sugeridas y temas en sidebar
- âœ… Responsive design
