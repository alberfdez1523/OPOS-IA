# ğŸµ MÃºsicaOpos AI â€” Asistente RAG para Oposiciones de MÃºsica

Chatbot inteligente con **Retrieval-Augmented Generation (RAG)** para preparar las oposiciones de Profesor de MÃºsica. Sube tus temarios en PDF, hazle preguntas a la IA, genera tests de autoevaluaciÃ³n y resÃºmenes descargables â€” todo desde una interfaz moderna y 100 % local.

> **Creado por Alberto FernÃ¡ndez**

---

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Frontend      â”‚â”€â”€â”€â”€â–¶â”‚     FastAPI Backend    â”‚â”€â”€â”€â”€â–¶â”‚    Ollama     â”‚
â”‚  HTML / CSS / JS  â”‚     â”‚                      â”‚     â”‚  Llama 3.1   â”‚
â”‚                  â”‚     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  Â· Chat          â”‚     â”‚  â”‚  FAISS Vector  â”‚  â”‚            â”‚
â”‚  Â· Tests         â”‚     â”‚  â”‚    Store       â”‚  â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Â· ResÃºmenes     â”‚     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚     â”‚    Groq â˜ï¸    â”‚
â”‚  Â· Login         â”‚     â”‚                      â”‚     â”‚  (fallback)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flujo RAG

1. **Pregunta** â†’ El usuario escribe una pregunta en el chat
2. **Retrieval** â†’ BÃºsqueda semÃ¡ntica en FAISS con embeddings `all-MiniLM-L6-v2`
3. **Augmentation** â†’ Los fragmentos mÃ¡s relevantes se inyectan como contexto al prompt
4. **Generation** â†’ Llama 3.1 (local vÃ­a Ollama) o Llama 3.3 70B (cloud vÃ­a Groq) genera la respuesta

---

## ğŸ“‹ Requisitos

| Componente | MÃ­nimo |
|------------|--------|
| Python | 3.11+ |
| RAM | ~8 GB (embeddings + FAISS + Llama 3.1 8B) |
| Ollama | Instalado con `llama3.1` descargado |
| Disco | ~6 GB (modelo + vectorstore) |

> **Nota:** Si prefieres usar Groq (cloud), solo necesitas una API key gratuita y no necesitas Ollama ni RAM extra.

---

## ğŸš€ InstalaciÃ³n

### 1. Instalar Ollama

```bash
# Windows: descargar desde https://ollama.com/download
# DespuÃ©s, descargar el modelo:
ollama pull llama3.1
```

### 2. Configurar el backend

```bash
cd backend

# (Opcional) Crear entorno virtual
python -m venv venv
venv\Scripts\activate          # Windows
# source venv/bin/activate     # Linux/Mac

# Instalar dependencias
pip install -r requirements.txt

# Configurar variables de entorno
copy .env.example .env         # Windows
# cp .env.example .env         # Linux/Mac
```

Editar `.env` segÃºn tu configuraciÃ³n (ver secciÃ³n [ConfiguraciÃ³n](#-configuraciÃ³n-env)).

### 3. AÃ±adir los PDFs del temario

```
backend/data/pdfs/
â”œâ”€â”€ TEMA 1 .pdf
â”œâ”€â”€ TEMA 2 .pdf
â”œâ”€â”€ ...
```

### 4. Ejecutar

```bash
cd backend
python -m uvicorn app.main:app --reload --port 8000
```

Al arrancar, el servidor **pre-carga** automÃ¡ticamente el modelo de embeddings y el vectorstore para que la primera consulta sea instantÃ¡nea.

### 5. Indexar documentos

Una vez el servidor estÃ© corriendo y los PDFs estÃ©n en `data/pdfs/`:

- **OpciÃ³n A:** Desde la interfaz web â†’ botÃ³n **"ğŸ”„ Indexar documentos"**
- **OpciÃ³n B:** Por API:
  ```bash
  curl -X POST http://localhost:8000/api/ingest
  ```

### 6. Abrir la web

```
http://localhost:8000
```

ContraseÃ±a de acceso: `musica2026`

---

## ğŸ“ Estructura del Proyecto

```
OPOS-IA/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py              # FastAPI app + startup preload
â”‚   â”‚   â”œâ”€â”€ config.py            # Settings (Pydantic)
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â””â”€â”€ routes.py        # 9 endpoints REST
â”‚   â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”‚   â”œâ”€â”€ embeddings.py    # HuggingFace embeddings (singleton)
â”‚   â”‚   â”‚   â”œâ”€â”€ vectorstore.py   # FAISS load/create/merge
â”‚   â”‚   â”‚   â”œâ”€â”€ retriever.py     # BÃºsqueda semÃ¡ntica
â”‚   â”‚   â”‚   â””â”€â”€ llm.py           # Ollama/Groq + test + summary
â”‚   â”‚   â””â”€â”€ ingestion/
â”‚   â”‚       â””â”€â”€ pdf_loader.py    # Carga y chunking de PDFs
â”‚   â”œâ”€â”€ data/pdfs/               # â† Temarios PDF aquÃ­
â”‚   â”œâ”€â”€ vectorstore/             # Ãndice FAISS (auto-generado)
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ login.html               # Pantalla de login
â”‚   â”œâ”€â”€ index.html               # Landing page
â”‚   â”œâ”€â”€ chat.html                # Chat con IA (streaming)
â”‚   â”œâ”€â”€ test.html                # Generador de tests
â”‚   â”œâ”€â”€ summary.html             # Generador de resÃºmenes + PDF
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ styles.css           # Estilos dark mode
â”‚   â””â”€â”€ js/
â”‚       â”œâ”€â”€ main.js              # JS landing
â”‚       â”œâ”€â”€ chat.js              # Chat + SSE streaming
â”‚       â”œâ”€â”€ test.js              # LÃ³gica de tests
â”‚       â””â”€â”€ summary.js           # ResÃºmenes + descarga PDF
â”œâ”€â”€ render.yaml                  # Deploy en Render (Groq)
â”œâ”€â”€ Procfile                     # Heroku/Render start command
â””â”€â”€ README.md
```

---

## ğŸ”Œ API Endpoints

| MÃ©todo | Ruta | DescripciÃ³n |
|--------|------|-------------|
| `GET` | `/api/stats` | EstadÃ­sticas (chunks, modelo, estado) |
| `POST` | `/api/ask` | Pregunta con respuesta completa |
| `POST` | `/api/ask/stream` | Pregunta con streaming SSE |
| `POST` | `/api/ingest` | Indexar todos los PDFs de `data/pdfs/` |
| `POST` | `/api/upload` | Subir un PDF nuevo |
| `GET` | `/api/pdfs` | Listar PDFs disponibles |
| `POST` | `/api/generate-test` | Generar test de N preguntas desde un tema |
| `POST` | `/api/generate-summary` | Generar resumen completo de un tema |
| `GET` | `/health` | Health check |

---

## âš™ï¸ ConfiguraciÃ³n (.env)

| Variable | Default | DescripciÃ³n |
|----------|---------|-------------|
| `LLM_PROVIDER` | `ollama` | Proveedor LLM: `ollama` (local) o `groq` (cloud) |
| `OLLAMA_BASE_URL` | `http://localhost:11434` | URL del servidor Ollama |
| `OLLAMA_MODEL` | `llama3.1` | Modelo local (8B, 4.6 GB) |
| `GROQ_API_KEY` | â€” | API key de Groq (solo si `LLM_PROVIDER=groq`) |
| `GROQ_MODEL` | `llama-3.3-70b-versatile` | Modelo cloud en Groq |
| `EMBEDDING_MODEL` | `sentence-transformers/all-MiniLM-L6-v2` | Modelo de embeddings |
| `CHUNK_SIZE` | `500` | TamaÃ±o de los fragmentos de texto |
| `CHUNK_OVERLAP` | `50` | Solapamiento entre chunks |

---

## ğŸ¨ Funcionalidades

### ğŸ’¬ Chat con IA
- Respuestas en streaming (Server-Sent Events)
- Renderizado de **Markdown** y **LaTeX** (KaTeX)
- Fuentes consultadas en panel **desplegable** (colapsado por defecto)
- BÃºsqueda semÃ¡ntica con FAISS sobre los temarios

### ğŸ“ Generador de Tests
- Selecciona un tema y la dificultad (medio / difÃ­cil)
- Genera tests de 3 a 20 preguntas tipo test con 4 opciones
- CorrecciÃ³n automÃ¡tica con puntuaciÃ³n y feedback

### ğŸ“„ Generador de ResÃºmenes
- Resumen completo y estructurado de cada tema
- Renderizado en Markdown con secciones, listas y negrita
- **Descarga instantÃ¡nea en PDF** con diseÃ±o profesional (impresiÃ³n nativa del navegador)

### ğŸ” Login
- Pantalla de acceso con contraseÃ±a
- SesiÃ³n guardada en `sessionStorage`

### âš¡ Rendimiento
- Pre-carga de embeddings y vectorstore al arrancar el servidor
- La primera consulta tras login es instantÃ¡nea
- Cache-busting automÃ¡tico en assets estÃ¡ticos

### ğŸ¨ DiseÃ±o
- Dark mode completo con gradientes pÃºrpura
- TipografÃ­a Inter + JetBrains Mono
- NavegaciÃ³n lateral entre Chat, Tests y ResÃºmenes
- Responsive design
- Subida de PDFs desde la interfaz

---

## â˜ï¸ Deploy en Render

El proyecto incluye `render.yaml` para despliegue automÃ¡tico en [Render](https://render.com) usando Groq como proveedor LLM (cloud):

1. Conectar el repositorio a Render
2. Configurar `GROQ_API_KEY` en el dashboard
3. El servicio arrancarÃ¡ automÃ¡ticamente con Llama 3.3 70B vÃ­a Groq

---

## ğŸ› ï¸ Stack TecnolÃ³gico

| Capa | TecnologÃ­a |
|------|------------|
| Backend | FastAPI Â· Uvicorn Â· LangChain |
| LLM | Ollama (local) Â· Groq (cloud) |
| Embeddings | HuggingFace `all-MiniLM-L6-v2` |
| Vector Store | FAISS |
| PDF Parsing | PyPDF |
| Frontend | HTML5 Â· CSS3 Â· Vanilla JS |
| Markdown | marked.js |
| MatemÃ¡ticas | KaTeX |
| PDF Export | ImpresiÃ³n nativa del navegador |
